from sklearn.metrics import classification_report

y_true = [0, 1, 0, 1, 1]
y_pred = [0, 1, 0, 0, 1]
target_names = ['class 0', 'class 1']
'''
准确率与召回率。
这2个值是评判分类准确率的一个重要标准。比如代码的最后将所有10个样本输入分类器进行测试的结果：

测试结果：array([ 0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.])
真实结果：array([ 0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.])

分为thin的准确率为0.83。是因为分类器分出了6个thin，其中正确的有5个，因此分为thin的准确率为5/6=0.83。
分为thin的召回率为1.00。是因为数据集中共有5个thin，而分类器把他们都分对了（虽然把一个fat分成了thin！），召回率5/5=1。
分为fat的准确率为1.00。不再赘述。
分为fat的召回率为0.80。是因为数据集中共有5个fat，而分类器只分出了4个（把一个fat分成了thin！），召回率4/5=0.80。

很多时候，尤其是数据分类难度较大的情况，准确率与召回率往往是矛盾的。
你可能需要根据你的需要找到最佳的一个平衡点。

F-score=(2*precision*recall)/(precision+recall)
'''
# 最后一行是用support加权平均算出来的
print(classification_report(y_true, y_pred, target_names=target_names))

'''
http://bookshadow.com/weblog/2014/06/10/precision-recall-f-measure/

准确率和召回率是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。其中精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率；召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率。
一般来说，Precision就是检索出来的条目（比如：文档、网页等）有多少是准确的，Recall就是所有准确的条目有多少被检索出来了。
正确率、召回率和 F 值是在鱼龙混杂的环境中，选出目标的重要评价指标。不妨看看这些指标的定义先：
    1. 正确率 = 提取出的正确信息条数 /  提取出的信息条数     
    2. 召回率 = 提取出的正确信息条数 /  样本中的信息条数    
        两者取值在0和1之间，数值越接近1，查准率或查全率就越高。   
    3. F值  = 正确率 * 召回率 * 2 / (正确率 + 召回率) （F 值即为正确率和召回率的调和平均值）
    
当然希望检索结果Precision越高越好，同时Recall也越高越好，但事实上这两者在某些情况下有矛盾的。
比如极端情况下，我们只搜索出了一个结果，且是准确的，那么Precision就是100%，但是Recall就很低；
而如果我们把所有结果都返回，那么比如Recall是100%，但是Precision就会很低。
因此在不同的场合中需要自己判断希望Precision比较高或是Recall比较高。
如果是做实验研究，可以绘制Precision-Recall曲线来帮助分析。

P和R指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure（又称为F-Score）。
F-Measure是Precision和Recall加权调和平均：
F = ((a^2+1)P*R)/((a^2)(P+R))
    当a=1时，是F1
'''
